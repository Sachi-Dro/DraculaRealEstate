{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing=pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting histograms\n",
    "# housing.hist(bins=50 , figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN-TEST SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_test(data,test_ratio):\n",
    "#     np.random.seed(42)\n",
    "#     shuffled=np.random.permutation(len(data))\n",
    "#     test_set_size=int(len(data)*test_ratio)\n",
    "#     test_indices=shuffled[:test_set_size]\n",
    "#     train_indices=shuffled[test_set_size:]\n",
    "#     return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "# train_set,test_set=split_train_test(housing,0.2)\n",
    "# print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above code can also be implemented as :-\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)\n",
    "# print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Smpling=>represnts whole population in test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index,test_index in split.split(housing,housing['CHAS']):\n",
    "#        strat_train_set=housing.loc[train_index]\n",
    "#        strat_test_set=housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat_train_set[\"CHAS\"].value_counts()\n",
    "# strat_test_set[\"CHAS\"].value_counts()\n",
    "# 94/7\n",
    "# 376/28\n",
    "#We can see that ratio of 0 and 1 are same in both training and testing data as per stratified shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing=strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOKING FOR CORRELATIONS SO THAT WE CAN REMOVE OUTLIERS THAT AFFECT OUR MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix=housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix[\"MEDV\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes=[\"MEDV\",\"RM\",\"ZN\",\"LSTAT\"]\n",
    "# scatter_matrix(housing[attributes],figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing.plot(kind=\"scatter\",x=\"RM\",y=\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTRIBUTE COMBINATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing[\"TAXRM\"]=housing[\"TAX\"]/housing[\"RM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing[\"TAXRM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_matrix=housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_matrix[\"MEDV\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing.plot(kind=\"scatter\",x=\"TAXRM\",y=\"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing=strat_train_set.drop(\"MEDV\",axis=1)\n",
    "# housing_labels=strat_train_set[\"MEDV\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISSING ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To take care of missing attributes we have 3 options :-\n",
    "#     GET RID OF MISSING DATA POINTS OF ALL ATTRIBUTES(depends on dataset size)\n",
    "#     GET RID OF WHOLE ATTRIBUTE FROM DATASET(correlation dekh kar karna hga)\n",
    "#     SET THE VALUE TO 0, MEAN OR MEDIAN(generally preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1(this is just a copy. orignal housing data remains unchnaged)\n",
    "# a=housing.dropna(subset=[\"RM\"])\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #option 2(we see no rm column is there and this is just a copy. orignal housing data remains unchnaged)\n",
    "# a=housing.drop(\"RM\",axis=1)\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 3(we fit with median value and this is just a copy. orignal housing data remains unchnaged)\n",
    "# median=housing[\"RM\"].median()\n",
    "# housing[\"RM\"].fillna(median)\n",
    "# housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing.describe()\n",
    "#to see the decription before imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## median is very important as it has to be automatically applied to all the missing values in training and testing data as well as any upcoming data. we have sklearn library to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer=SimpleImputer(strategy=\"median\")\n",
    "# imputer.fit(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer.statistics_.shape\n",
    "#it finds median of all the 15 cols so that kisi me bhi missing value ho to ye median usme fit ho jae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=imputer.transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_tr=pd.DataFrame(X,columns=housing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primarily 3 types  of object hai sklearn me:-\n",
    "#     1.ESTIMATORS-estimates some parameters based on a dataset. eg: imputers.It has a fit() method that fits the dataset accordingly\n",
    "#     and calculates the internal parameters.\n",
    "    \n",
    "#     2.TRANSFORMERS-Some estimator work as transformer.It takes input and return output based on learning from fit. It also has a \n",
    "#       convenience function ie fit_transform() which fits and then transforms.It is optimised to run together\n",
    "    \n",
    "#     3.PREDICTORS-Linear Regressors, KNN, are examples of predictor. They have fit() and predict(). It also gives score functions\n",
    "#     that evaluates the score of goodness of predictions made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Scaling because generally all algo works well when all the feature values are on a same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Types of feature scaling methods primarily:-\n",
    "#     1. MIN-MAX SCALING(NORMALISATION)- (VALUE-MIN)/(MAX-MIN)(gives value between 0 to 1). We do this via MinMaxScaler\n",
    "#     2. STANDARDISATION- (VALUE-MEAN)/(STANDARD DEVIATION)(variance=1). We do this via StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A PIPELINE-A CODE THAT CAN BE CHANGED EASILY AND THE MODEL CHANGES AUTOMATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# my_pipleline=Pipeline([\n",
    "#     ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "#     #... add as many as you want\n",
    "#     ('std_scaler',StandardScaler())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_num_tr=my_pipleline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_num_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# # model=LinearRegression()\n",
    "# # model=DecisionTreeRegressor()\n",
    "# model=RandomForestRegressor()\n",
    "# model.fit(housing_num_tr,housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_data=housing.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_labels=housing_labels.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared_data=my_pipleline.transform(some_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(some_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import  mean_squared_error\n",
    "# housing_predictions=model.predict(housing_num_tr)\n",
    "# mse=mean_squared_error(housing_labels,housing_predictions)\n",
    "# rmse=np.sqrt(mse)\n",
    "# rmse\n",
    "\n",
    "\n",
    "\n",
    "# this linear regression model gives a lot of error so we discard this model and use decision tree regressor but decision tree\n",
    "#gives overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(model,housing_num_tr,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "rmse=np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.80902454, 2.56400132, 5.13971187, 3.10062673, 3.08174961,\n",
       "       2.0031091 , 3.30268443, 3.48339344, 2.12404257, 3.94501676])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(scores):\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Mean: {scores.mean()}\")\n",
    "    print(f\"STD: {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [3.80902454 2.56400132 5.13971187 3.10062673 3.08174961 2.0031091\n",
      " 3.30268443 3.48339344 2.12404257 3.94501676]\n",
      "Mean: 3.2553360366745436\n",
      "STD: 0.8813207650390685\n"
     ]
    }
   ],
   "source": [
    "print_scores(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dracula.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model,\"Dracula.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.129 22.195 47.085 32.101 45.409 34.182 20.757 24.041 32.989 19.978\n",
      " 19.374 29.298 22.072 32.199 20.389 19.097 12.81  20.802 27.647 19.658\n",
      " 19.903 45.827 12.082 14.497 25.748 33.225 16.031 15.258  7.042 19.934\n",
      " 23.732 23.287 17.526 14.726 21.121 18.694 22.148 17.34  45.886 17.575\n",
      " 21.179 19.085 19.548 18.508 32.855  8.47  25.075 14.46  20.52  21.583\n",
      " 47.037 24.59  15.103 21.82  19.767 46.578 32.917 20.293 34.14  10.447\n",
      " 23.695 36.15  32.88  23.856 13.941 21.224 20.405 15.814 28.038 24.569\n",
      " 23.843 32.11  19.361 31.186 10.675 20.444 41.993 19.573 19.9   15.289\n",
      " 42.119  9.332 22.897 22.622 27.714 16.317 23.027 22.566 20.388 18.78\n",
      " 26.518 11.015 33.664 12.856 25.675 20.085 33.061 14.031 20.66  21.197\n",
      " 20.166] [24.6, 22.0, 44.8, 23.6, 48.8, 36.5, 19.7, 23.1, 34.6, 21.5, 23.1, 15.0, 23.0, 34.9, 18.5, 10.4, 10.2, 18.9, 23.9, 19.3, 19.4, 48.3, 10.9, 13.6, 27.5, 37.3, 16.1, 15.2, 10.5, 21.4, 23.2, 20.7, 21.7, 13.0, 22.3, 19.6, 21.2, 18.1, 50.0, 23.7, 22.6, 20.5, 18.9, 19.5, 32.7, 8.8, 29.1, 19.0, 22.6, 21.2, 50.0, 22.5, 17.8, 20.3, 20.4, 37.6, 35.4, 20.4, 33.3, 12.1, 23.1, 37.9, 36.1, 23.7, 13.1, 23.8, 19.6, 13.1, 27.9, 27.0, 22.9, 31.7, 17.1, 30.3, 8.1, 19.6, 44.0, 19.5, 18.5, 17.2, 35.2, 8.3, 21.6, 20.5, 23.7, 14.2, 22.8, 20.6, 19.6, 19.6, 23.9, 6.3, 32.0, 13.4, 22.0, 19.9, 28.7, 19.1, 23.4, 11.9, 21.7]\n"
     ]
    }
   ],
   "source": [
    "# X_test=strat_test_set.drop(\"MEDV\",axis=1)\n",
    "# Y_test=strat_test_set[\"MEDV\"].copy()\n",
    "# X_test_prepared=my_pipleline.transform(X_test)\n",
    "# final_predictions=model.predict(X_test_prepared)\n",
    "# final_mse=mean_squared_error(Y_test,final_predictions)\n",
    "# final_rmse=np.sqrt(final_mse)\n",
    "# print(final_predictions,list(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2219481023532053"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44214569,  3.18549186, -1.12157806, -0.27288841, -1.42038605,\n",
       "       -0.5507194 , -1.74081446,  2.56373713, -0.9938038 , -0.57365513,\n",
       "       -0.98653542,  0.43847588, -0.49179946])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepared_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.557])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from joblib import dump,load\n",
    "# import numpy as np\n",
    "# model=load(\"Dracula.joblib\")\n",
    "# features=np.array([-0.44214569,  3.18549186, -1.12157806, -0.27288841, -1.42038605,\n",
    "#        -0.5507194 , -1.74081446,  2.56373713, -0.9938038 , -0.57365513,\n",
    "#        -0.98653542,  0.43847588, -0.49179946])\n",
    "# model.predict([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
